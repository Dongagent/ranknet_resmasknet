{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from resmasknet_test import *\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resmasking_dropout1(in_channels=3, num_classes=7, weight_path=\"\"):\n",
    "    model = ResMasking(weight_path)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, 7)\n",
    "        # nn.Linear(512, num_classes)\n",
    "    )\n",
    "    def get_resource_path():\n",
    "        return ''\n",
    "\n",
    "    # use_gpu = torch.cuda.is_available()\n",
    "    # if use_gpu:\n",
    "    #     model.load_state_dict(\n",
    "    #         torch.load(\n",
    "    #             os.path.join(\n",
    "    #                 get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "    #                 )\n",
    "    #             )['net']\n",
    "    #         )\n",
    "    #     model.cuda()\n",
    "\n",
    "    # else:\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            os.path.join(\n",
    "                get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "            ),\n",
    "        map_location={\"cuda:0\": \"cpu\"},\n",
    "        )['net']\n",
    "    )\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, 1)\n",
    "        # nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class SiameseRankNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseRankNet, self).__init__()\n",
    "        # Load ResMaskNet model\n",
    "        self.model = resmasking_dropout1(in_channels=3, num_classes=7)\n",
    "\n",
    "        \n",
    "        # Define the fully connected layers on top of concatenated feature vectors\n",
    "        # self.model.fc = nn.Sequential(\n",
    "        #     nn.Dropout(0.4),\n",
    "        #     nn.Linear(512, 1)\n",
    "        # )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.activation = nn.Tanh()\n",
    "    \n",
    "    # _once\n",
    "    def forward_once(self, x):\n",
    "        # Forward pass through ResMaskNet\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Pass each input image through ResMaskNet to obtain feature vectors\n",
    "        x1 = self.forward_once(x1)\n",
    "        x2 = self.forward_once(x2)\n",
    "\n",
    "        # Concatenate the feature vectors\n",
    "        # x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Pass the concatenated feature vector through the fully connected layers\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        # Pass the output through sigmoid to obtain the probability of the input images being similar\n",
    "        # normalize x1 - x2 as a probability that x1 should rank higher than x2\n",
    "        x = self.sigmoid(x1 - x2)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "aver_4_sorted_data = ['ha_212.png', 'ha_393.png', 'ha_428.png', 'ha_489.png', 'ha_412.png', 'ha_202.png', 'ha_348.png', 'ha_24.png', 'ha_407.png', 'ha_288.png', 'ha_367.png', 'ha_341.png', 'ha_235.png', 'ha_443.png', 'ha_450.png', 'ha_185.png', 'ha_50.png', 'ha_491.png', 'ha_301.png', 'ha_11.png', 'ha_422.png', 'ha_130.png', 'ha_243.png', 'ha_201.png', 'ha_32.png', 'ha_19.png', 'ha_384.png', 'ha_184.png', 'ha_311.png', 'ha_497.png', 'ha_256.png', 'ha_27.png', 'ha_107.png', 'ha_268.png', 'ha_329.png', 'ha_315.png', 'ha_2.png', 'ha_368.png', 'ha_241.png', 'ha_303.png', 'ha_221.png', 'ha_151.png', 'ha_342.png', 'ha_296.png', 'ha_152.png', 'ha_442.png', 'ha_186.png', 'ha_344.png', 'ha_215.png', 'ha_320.png', 'ha_149.png', 'ha_122.png', 'ha_54.png', 'ha_476.png', 'ha_106.png', 'ha_249.png', 'ha_132.png', 'ha_33.png', 'ha_207.png', 'ha_451.png', 'ha_172.png', 'ha_244.png', 'ha_454.png', 'ha_43.png', 'ha_131.png', 'ha_377.png', 'ha_396.png', 'ha_284.png', 'ha_59.png', 'ha_1.png', 'ha_252.png', 'ha_466.png', 'ha_110.png', 'ha_404.png', 'ha_292.png', 'ha_124.png', 'ha_482.png', 'ha_477.png', 'ha_5.png', 'ha_382.png', 'ha_9.png', 'ha_334.png', 'ha_381.png', 'ha_111.png', 'ha_380.png', 'ha_310.png', 'ha_475.png', 'ha_128.png', 'ha_314.png', 'ha_262.png', 'ha_174.png', 'ha_119.png', 'ha_139.png', 'ha_257.png', 'ha_233.png', 'ha_116.png', 'ha_399.png', 'ha_84.png', 'ha_145.png', 'ha_16.png']\n",
    "base_folder = 'data/happiness_selected_imgonly100/'\n",
    "data_path = [base_folder + i for i in aver_4_sorted_data]\n",
    "\n",
    "def generate_dataset(data_path):\n",
    "    dataset = []\n",
    "    for i in range(len(data_path)):\n",
    "        for j in range(i+1, len(data_path)):\n",
    "            dataset.append([[data_path[i], data_path[j]], 1])\n",
    "    return dataset\n",
    "\n",
    "raw_data = generate_dataset(data_path)\n",
    "# print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "class PairwiseRatingDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, mode='train'):\n",
    "        \n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "        # compute pairs and labels\n",
    "        self.pairs = [i[0] for i in self.data]\n",
    "        self.labels = [i[1] for i in self.data]\n",
    "\n",
    "        # self.pairs = self.load_image_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = Image.open(self.pairs[idx][0])\n",
    "        img2 = Image.open(self.pairs[idx][1])\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, self.labels[idx]\n",
    "\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.pairs[idx][0], self.pairs[idx][1], self.labels[idx]\n",
    "\n",
    "    # def load_image_data(self):\n",
    "    #     print('loading image data...')\n",
    "    #     # Load images and label for a given index\n",
    "    #     image_pairs = []\n",
    "        \n",
    "    #     for i in range(self.__len__()):\n",
    "    #         img1 = Image.open(self.pairs[i][0])\n",
    "    #         img2 = Image.open(self.pairs[i][1])\n",
    "\n",
    "    #         # Apply transformations if any\n",
    "    #         if self.transform:\n",
    "    #             img1 = self.transform(img1)\n",
    "    #             img2 = self.transform(img2)\n",
    "            \n",
    "    #         image_pairs.append([img1, img2])\n",
    "\n",
    "    #     return image_pairs\n",
    "        \n",
    "\n",
    "# Define transformations to be applied to images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img1, train_img2, train_labels = next(iter(dataloader))\n",
    "# print(train_img1.shape, train_img2.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_to_img((train_img1[0]*0.5 + 0.5) * 255, mode='RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_to_img = transforms.Compose([\n",
    "#     transforms.ToPILImage(mode='RGB'),\n",
    "#     ]\n",
    "# )\n",
    "# for i in range(len(train_img1)):\n",
    "#     print(img1)\n",
    "#     img1 = transform_to_img(train_img1[i])\n",
    "#     img2 = transform_to_img(train_img2[i])\n",
    "#     # img1.show()\n",
    "#     # img2.show()\n",
    "#     print(train_labels[i])\n",
    "#     break\n",
    "#     # input()\n",
    "# # train_img1[0].shape\n",
    "# # transform_to_img(train_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "# train_dataloader = DataLoader(mydataset['train'], batch_size=32, shuffle=True)\n",
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseRankNet()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# dataset = PairwiseRatingDataset(raw_data, transform=transform)\n",
    "\n",
    "# Split data into train, val sets\n",
    "num_data = len(raw_data)\n",
    "num_train = int(0.8 * num_data)\n",
    "num_val = num_data - num_train\n",
    "\n",
    "# Create indices for train and val sets\n",
    "indices = list(range(num_data))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[:num_train]\n",
    "val_indices = indices[num_train:]\n",
    "\n",
    "# Create train and val datasets by indexing the PairwiseRatingDataset instance\n",
    "train_dataset = [raw_data[i] for i in train_indices]\n",
    "val_dataset = [raw_data[i] for i in val_indices]\n",
    "train_dataset = PairwiseRatingDataset(train_dataset, transform=transform)\n",
    "val_dataset = PairwiseRatingDataset(val_dataset, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# Create DataLoader instances for train and val sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, sample  in enumerate(train_dataloader):\n",
    "    print(sample[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radam import *\n",
    "\n",
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        batch_size = target.size(0)\n",
    "        # pred = torch.argmax(output, dim=1) # return the index of the max value in output\n",
    "        # correct = pred.eq(target).float().sum(0)\n",
    "        correct = ((target - output) < 0.05).sum(0)\n",
    "        acc = correct * 100 / batch_size # acc percentage\n",
    "        # print('acc', acc)\n",
    "    return [acc]\n",
    "\n",
    "# start training\n",
    "model.train()\n",
    "lr = 0.0001\n",
    "weight_decay = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "def GetLoss(model, batch):\n",
    "    batch = {k:v.to(model.device) for k, v in batch.items()}\n",
    "    print(batch)\n",
    "#     out = model(x1 = batch[])\n",
    "\n",
    "optimizer = RAdam(\n",
    "            params=model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch official tutorial (but modified a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    train_acc = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=False):\n",
    "        \n",
    "        imgs1, imgs2, labels = data[0].cuda(non_blocking=True), data[1].cuda(non_blocking=True), data[2].cuda(non_blocking=True)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        labels = labels.float()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs1, imgs2)\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)[0]\n",
    "        acc = acc.sum() / len(acc)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "        if i % int(len(train_dataloader) / 4) == int(len(train_dataloader) / 4) - 1:\n",
    "            last_loss = running_loss / int(len(train_dataloader) / 4) # loss per 1/4 batch\n",
    "            last_acc = train_acc / int(len(train_dataloader) / 4)\n",
    "            print(' batch {} loss: {}, acc: {}'.format(i+1, last_loss, last_acc))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('Acc/train', last_acc, tb_x)\n",
    "            running_loss = 0.\n",
    "            train_acc = 0.\n",
    "    return last_loss, last_acc\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "# Define the path and name of your log file\n",
    "logfile = timestamp + \"log.json\"\n",
    "\n",
    "# Define an empty dictionary object to store your log data\n",
    "logdata = {}\n",
    "\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "best_vacc = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/124 [00:00<?, ?it/s]/home/src/ranknet_resmasknet/radam.py:45: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      " 25%|██▌       | 31/124 [00:28<01:22,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 31 loss: 0.7350281784611363, acc: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 62/124 [00:56<00:55,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 62 loss: 0.6332290403304561, acc: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 93/124 [01:23<00:27,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 93 loss: 0.45378732681274414, acc: 0.10080645161290322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 124 loss: 0.25941455292124904, acc: 2.1169354838709675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.25941455292124904 valid 0.692278265953064\n",
      "ACC train 2.1169354838709675 valid 0.0\n",
      "EPOCH 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 31/124 [00:42<01:28,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 31 loss: 0.10424023950772901, acc: 27.52016129032258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 62/124 [01:11<00:55,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 62 loss: 0.04596229228040864, acc: 70.46370967741936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 93/124 [01:39<00:27,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 93 loss: 0.04634321228631081, acc: 79.63709677419355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 124 loss: 0.03381656918434366, acc: 87.56720438311177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.03381656918434366 valid 0.9341935515403748\n",
      "ACC train 87.56720438311177 valid 0.0\n",
      "EPOCH 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 31/124 [00:27<01:22,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 31 loss: 0.028736904954477664, acc: 87.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 62/124 [00:54<00:54,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 62 loss: 0.0177717165150229, acc: 94.25403225806451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 93/124 [01:21<00:26,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 93 loss: 0.007056525052194634, acc: 99.8991935483871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 124 loss: 0.007666808425358707, acc: 99.09274193548387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.007666808425358707 valid 0.9043341875076294\n",
      "ACC train 99.09274193548387 valid 0.0\n",
      "EPOCH 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 31/124 [00:26<01:20,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 31 loss: 0.0076116035437031136, acc: 98.58870967741936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 62/124 [00:54<00:53,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 62 loss: 0.006732168021581827, acc: 98.28629032258064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 93/124 [01:21<00:26,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 93 loss: 0.0032320367848320355, acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 124 loss: 0.002927155305270947, acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.002927155305270947 valid 0.8160174489021301\n",
      "ACC train 100.0 valid 0.0\n",
      "EPOCH 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 6\u001b[0m avg_loss, avg_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m      8\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     imgs1, imgs2, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), data[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mPairwiseRatingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     28\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img1)\n\u001b[0;32m---> 29\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img1, img2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torchvision/transforms/transforms.py:346\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torchvision/transforms/functional.py:474\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    472\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    473\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py:252\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pybert/lib/python3.8/site-packages/PIL/Image.py:2115\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2107\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2108\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2109\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2110\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2111\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2112\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2113\u001b[0m         )\n\u001b[0;32m-> 2115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train()\n",
    "    avg_loss, avg_acc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.eval()\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    \n",
    "    # eval\n",
    "    vacc = 0.\n",
    "    for i, vdata in tqdm(enumerate(val_dataloader), total=len(val_dataloader), leave=False):\n",
    "        with torch.no_grad():\n",
    "            vimgs1, vimgs2, vlabels = vdata[0].cuda(non_blocking=True), vdata[1].cuda(non_blocking=True), vdata[2].cuda(non_blocking=True)\n",
    "            vlabels = vlabels.unsqueeze(1)\n",
    "            vlabels = vlabels.float()\n",
    "\n",
    "            voutputs = model(vimgs1, vimgs2)\n",
    "            vloss = loss_func(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "            acc = accuracy(voutputs, vlabels)[0]\n",
    "            acc = acc.sum() / len(acc)\n",
    "            vacc += acc\n",
    "    avg_vacc = vacc / (i + 1)\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Result of EPOCH', epoch_number + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('ACC train {} valid {}'.format(avg_acc, avg_vacc))\n",
    "    \n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation Acc',\n",
    "                    { 'Training' : avg_acc, 'Validation' : avg_vacc },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    # use acc instead\n",
    "    if avg_vacc > best_vacc:\n",
    "        best_vacc = avg_vacc\n",
    "        model_path = 'model_{}_epoch{}.pt'.format(timestamp, epoch_number + 1)\n",
    "        if not os.path.exists('check_points'):\n",
    "            os.mkdir('check_points')\n",
    "            \n",
    "        model_path = os.path.join('check_points', model_path)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    # if avg_vloss < best_vloss:\n",
    "    #     best_vloss = avg_vloss\n",
    "    #     model_path = 'model_{}_epoch{}'.format(timestamp, epoch_number + 1)\n",
    "    #     torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "    \n",
    "    \n",
    "    # Store the values in a sub-dictionary with epoch number as key\n",
    "    logdata[epoch] = {\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"train_acc\": avg_acc,\n",
    "        \"val_loss\": avg_vloss,\n",
    "        \"val_acc\": avg_vacc\n",
    "    }\n",
    "\n",
    "        \n",
    "# Write the dictionary object to your log file as JSON\n",
    "with open(logfile, \"w\") as f:\n",
    "    json.dump(logdata, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('check_points'):\n",
    "    os.mkdir('check_points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train ver1\n",
    "# model.train() \n",
    "# train_loss = 0.0\n",
    "# train_acc = 0.0\n",
    "# for batch_idx, sample in tqdm(\n",
    "#     enumerate(train_dataloader), total=len(train_dataloader), leave=False\n",
    "# ):\n",
    "#     batch_img1, batch_img2, targets = sample[0].cuda(non_blocking=True), sample[1].cuda(non_blocking=True), sample[2].cuda(non_blocking=True) # or something similar\n",
    "#     targets = targets.unsqueeze(1)\n",
    "#     targets = targets.float()\n",
    "    \n",
    "#     # compute output, measure accuracy and record loss\n",
    "#     outputs = model(batch_img1, batch_img2)\n",
    "#     loss = loss_func(outputs, targets)\n",
    "#     acc = accuracy(outputs, targets)[0]\n",
    "#     acc = acc.sum() / len(acc)\n",
    "    \n",
    "#     # acc = eval_metrics(targets, outputs, 2)[0]\n",
    "\n",
    "#     break\n",
    "#     train_loss += loss.item()\n",
    "#     train_acc += acc.item()\n",
    "\n",
    "    \n",
    "#     # compute gradient and do SGD step\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybert",
   "language": "python",
   "name": "pybert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "92b0ced2bda8a9d74b5fcf92aa8bdcb1cdcbfa04a2239101742f1cb36a3c525b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
