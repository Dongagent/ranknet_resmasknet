{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pybert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from resmasknet_test import *\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resmasking_dropout1(in_channels=3, num_classes=7, weight_path=\"\"):\n",
    "    model = ResMasking(weight_path)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, 7)\n",
    "        # nn.Linear(512, num_classes)\n",
    "    )\n",
    "    def get_resource_path():\n",
    "        return ''\n",
    "\n",
    "    # use_gpu = torch.cuda.is_available()\n",
    "    # if use_gpu:\n",
    "    #     model.load_state_dict(\n",
    "    #         torch.load(\n",
    "    #             os.path.join(\n",
    "    #                 get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "    #                 )\n",
    "    #             )['net']\n",
    "    #         )\n",
    "    #     model.cuda()\n",
    "\n",
    "    # else:\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            os.path.join(\n",
    "                get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "            ),\n",
    "        map_location={\"cuda:0\": \"cpu\"},\n",
    "        )['net']\n",
    "    )\n",
    "    # model.fc = nn.Sequential(\n",
    "    #     nn.Dropout(0.4),\n",
    "    #     nn.Linear(512, 1)\n",
    "    #     # nn.Linear(512, num_classes)\n",
    "    # )\n",
    "    return model\n",
    "\n",
    "class SiameseRankNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseRankNet, self).__init__()\n",
    "        # Load ResMaskNet model\n",
    "        self.model = resmasking_dropout1(in_channels=3, num_classes=7)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        # Define the fully connected layers on top of concatenated feature vectors\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128), # modify from (512, 7)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 7)\n",
    "            \n",
    "        )\n",
    "        self.FER_2013_EMO_DICT = {\n",
    "            0: \"angry\",\n",
    "            1: \"disgust\",\n",
    "            2: \"fear\",\n",
    "            3: \"happy\",\n",
    "            4: \"sad\",\n",
    "            5: \"surprise\",\n",
    "            6: \"neutral\",\n",
    "        }\n",
    "        self.FER_2013_EMONUM = {v:k for k, v in self.FER_2013_EMO_DICT.items()}\n",
    "        self.emotion = 'happy'\n",
    "        self.idx = self.FER_2013_EMONUM[self.emotion]\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "        # self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    # _once\n",
    "    def forward_once(self, x):\n",
    "        # Forward pass through ResMaskNet\n",
    "        x = self.model(x)\n",
    "        # x = x.view(x.size()[0], -1)\n",
    "        \n",
    "        # modify\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Pass each input image through ResMaskNet to obtain feature vectors\n",
    "        x1 = self.forward_once(x1)\n",
    "        x2 = self.forward_once(x2)\n",
    "        \n",
    "        # get target emotion idx\n",
    "        x1 = x1[:, self.idx]\n",
    "        x2 = x2[:, self.idx]\n",
    "\n",
    "        # Concatenate the feature vectors\n",
    "        # x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Pass the concatenated feature vector through the fully connected layers\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        # Pass the output through sigmoid to obtain the probability of the input images being similar\n",
    "        # normalize x1 - x2 as a probability that x1 should rank higher than x2\n",
    "        x = self.sigmoid(x1 - x2)\n",
    "        return x.unsqueeze(1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "aver_4_sorted_data = ['ha_212.png', 'ha_393.png', 'ha_428.png', 'ha_489.png', 'ha_412.png', 'ha_202.png', 'ha_348.png', 'ha_24.png', 'ha_407.png', 'ha_288.png', 'ha_367.png', 'ha_341.png', 'ha_235.png', 'ha_443.png', 'ha_450.png', 'ha_185.png', 'ha_50.png', 'ha_491.png', 'ha_301.png', 'ha_11.png', 'ha_422.png', 'ha_130.png', 'ha_243.png', 'ha_201.png', 'ha_32.png', 'ha_19.png', 'ha_384.png', 'ha_184.png', 'ha_311.png', 'ha_497.png', 'ha_256.png', 'ha_27.png', 'ha_107.png', 'ha_268.png', 'ha_329.png', 'ha_315.png', 'ha_2.png', 'ha_368.png', 'ha_241.png', 'ha_303.png', 'ha_221.png', 'ha_151.png', 'ha_342.png', 'ha_296.png', 'ha_152.png', 'ha_442.png', 'ha_186.png', 'ha_344.png', 'ha_215.png', 'ha_320.png', 'ha_149.png', 'ha_122.png', 'ha_54.png', 'ha_476.png', 'ha_106.png', 'ha_249.png', 'ha_132.png', 'ha_33.png', 'ha_207.png', 'ha_451.png', 'ha_172.png', 'ha_244.png', 'ha_454.png', 'ha_43.png', 'ha_131.png', 'ha_377.png', 'ha_396.png', 'ha_284.png', 'ha_59.png', 'ha_1.png', 'ha_252.png', 'ha_466.png', 'ha_110.png', 'ha_404.png', 'ha_292.png', 'ha_124.png', 'ha_482.png', 'ha_477.png', 'ha_5.png', 'ha_382.png', 'ha_9.png', 'ha_334.png', 'ha_381.png', 'ha_111.png', 'ha_380.png', 'ha_310.png', 'ha_475.png', 'ha_128.png', 'ha_314.png', 'ha_262.png', 'ha_174.png', 'ha_119.png', 'ha_139.png', 'ha_257.png', 'ha_233.png', 'ha_116.png', 'ha_399.png', 'ha_84.png', 'ha_145.png', 'ha_16.png']\n",
    "base_folder = 'data/happiness_selected_imgonly100/'\n",
    "data_path = [base_folder + i for i in aver_4_sorted_data]\n",
    "\n",
    "def generate_dataset(data_path):\n",
    "    dataset = []\n",
    "    for i in range(len(data_path)):\n",
    "        for j in range(i+1, len(data_path)):\n",
    "            dataset.append([[data_path[i], data_path[j]], 1])\n",
    "    return dataset\n",
    "\n",
    "raw_data = generate_dataset(data_path)\n",
    "# print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "class PairwiseRatingDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, mode='train'):\n",
    "        \n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "        # compute pairs and labels\n",
    "        self.pairs = [i[0] for i in self.data]\n",
    "        self.labels = [i[1] for i in self.data]\n",
    "\n",
    "        # self.pairs = self.load_image_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = Image.open(self.pairs[idx][0])\n",
    "        img2 = Image.open(self.pairs[idx][1])\n",
    "        \n",
    "        # pre computed face box\n",
    "        start_x, start_y, end_x, end_y = 193, 114, 442, 363\n",
    "    \n",
    "        img1 = img1.crop([start_x, start_y, end_x, end_y])\n",
    "        img2 = img2.crop([start_x, start_y, end_x, end_y])\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, self.labels[idx]\n",
    "\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     return self.pairs[idx][0], self.pairs[idx][1], self.labels[idx]\n",
    "\n",
    "    # def load_image_data(self):\n",
    "    #     print('loading image data...')\n",
    "    #     # Load images and label for a given index\n",
    "    #     image_pairs = []\n",
    "        \n",
    "    #     for i in range(self.__len__()):\n",
    "    #         img1 = Image.open(self.pairs[i][0])\n",
    "    #         img2 = Image.open(self.pairs[i][1])\n",
    "\n",
    "    #         # Apply transformations if any\n",
    "    #         if self.transform:\n",
    "    #             img1 = self.transform(img1)\n",
    "    #             img2 = self.transform(img2)\n",
    "            \n",
    "    #         image_pairs.append([img1, img2])\n",
    "\n",
    "    #     return image_pairs\n",
    "        \n",
    "\n",
    "# Define transformations to be applied to images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img1, train_img2, train_labels = next(iter(dataloader))\n",
    "# print(train_img1.shape, train_img2.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_to_img((train_img1[0]*0.5 + 0.5) * 255, mode='RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_to_img = transforms.Compose([\n",
    "#     transforms.ToPILImage(mode='RGB'),\n",
    "#     ]\n",
    "# )\n",
    "# for i in range(len(train_img1)):\n",
    "#     print(img1)\n",
    "#     img1 = transform_to_img(train_img1[i])\n",
    "#     img2 = transform_to_img(train_img2[i])\n",
    "#     # img1.show()\n",
    "#     # img2.show()\n",
    "#     print(train_labels[i])\n",
    "#     break\n",
    "#     # input()\n",
    "# # train_img1[0].shape\n",
    "# # transform_to_img(train_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "# train_dataloader = DataLoader(mydataset['train'], batch_size=32, shuffle=True)\n",
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseRankNet()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# dataset = PairwiseRatingDataset(raw_data, transform=transform)\n",
    "\n",
    "# Split data into train, val sets\n",
    "num_data = len(raw_data)\n",
    "num_train = int(0.8 * num_data)\n",
    "num_val = num_data - num_train\n",
    "\n",
    "# Create indices for train and val sets\n",
    "indices = list(range(num_data))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[:num_train]\n",
    "val_indices = indices[num_train:]\n",
    "\n",
    "# Create train and val datasets by indexing the PairwiseRatingDataset instance\n",
    "train_dataset = [raw_data[i] for i in train_indices]\n",
    "val_dataset = [raw_data[i] for i in val_indices]\n",
    "train_dataset = PairwiseRatingDataset(train_dataset, transform=transform)\n",
    "val_dataset = PairwiseRatingDataset(val_dataset, transform=transform)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# Create DataLoader instances for train and val sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, sample  in enumerate(train_dataloader):\n",
    "    print(sample[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radam import *\n",
    "\n",
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        batch_size = target.size(0)\n",
    "        # pred = torch.argmax(output, dim=1) # return the index of the max value in output\n",
    "        # correct = pred.eq(target).float().sum(0)\n",
    "        correct = (output > 0.5).sum(0)\n",
    "        # print(batch_size)\n",
    "        acc = correct * 100 / batch_size # acc percentage\n",
    "        # print('acc', acc)\n",
    "    return [acc]\n",
    "\n",
    "# start training\n",
    "model.train()\n",
    "lr = 0.001\n",
    "weight_decay = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "def GetLoss(model, batch):\n",
    "    batch = {k:v.to(model.device) for k, v in batch.items()}\n",
    "    print(batch)\n",
    "#     out = model(x1 = batch[])\n",
    "\n",
    "optimizer = RAdam(\n",
    "            params=model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch official tutorial (but modified a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    train_acc = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader), leave=False):\n",
    "        \n",
    "        imgs1, imgs2, labels = data[0].cuda(non_blocking=True), data[1].cuda(non_blocking=True), data[2].cuda(non_blocking=True)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        labels = labels.float()\n",
    "        # print(labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs1, imgs2)\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)[0]\n",
    "        acc = acc.sum() / len(acc)        \n",
    "\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "        if i % int(len(train_dataloader) / 4) == int(len(train_dataloader) / 4) - 1:\n",
    "            last_loss = running_loss / int(len(train_dataloader) / 4) # loss per 1/4 batch\n",
    "            last_acc = train_acc / int(len(train_dataloader) / 4)\n",
    "            print(' batch {} loss: {}, acc: {}'.format(i+1, last_loss, last_acc))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('Acc/train', last_acc, tb_x)\n",
    "            running_loss = 0.\n",
    "            train_acc = 0.\n",
    "    return last_loss, last_acc\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "# Define the path and name of your log file\n",
    "logfile = timestamp + \"log.json\"\n",
    "\n",
    "# Define an empty dictionary object to store your log data\n",
    "logdata = {}\n",
    "\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "best_vacc = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 31/124 [00:23<01:08,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 31 loss: 0.6741705652206175, acc: 58.16532258064516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 62/124 [00:45<00:44,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 62 loss: 0.5612690919829953, acc: 71.47177419354838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 93/124 [01:08<00:20,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 93 loss: 0.4343926291311941, acc: 83.06451612903226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch 124 loss: 0.38553050448817594, acc: 84.24059147988596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 23/62 [00:07<00:12,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('\\nEPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train()\n",
    "    avg_loss, avg_acc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.eval()\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    \n",
    "    # eval\n",
    "    vacc = 0.\n",
    "    for i, vdata in tqdm(enumerate(val_dataloader), total=len(val_dataloader), leave=False):\n",
    "        with torch.no_grad():\n",
    "            vimgs1, vimgs2, vlabels = vdata[0].cuda(non_blocking=True), vdata[1].cuda(non_blocking=True), vdata[2].cuda(non_blocking=True)\n",
    "            vlabels = vlabels.unsqueeze(1)\n",
    "            vlabels = vlabels.float()\n",
    "\n",
    "            voutputs = model(vimgs1, vimgs2)\n",
    "            vloss = loss_func(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "            acc = accuracy(voutputs, vlabels)[0]\n",
    "            acc = acc.sum() / len(acc)\n",
    "            vacc += acc\n",
    "    avg_vacc = vacc / (i + 1)\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Result of EPOCH', epoch_number + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('ACC train {} valid {}'.format(avg_acc, avg_vacc))\n",
    "    \n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation Acc',\n",
    "                    { 'Training' : avg_acc, 'Validation' : avg_vacc },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    # use acc instead\n",
    "    if avg_vacc > best_vacc:\n",
    "        best_vacc = avg_vacc\n",
    "        model_path = 'model_{}_epoch{}.pt'.format(timestamp, epoch_number + 1)\n",
    "        if not os.path.exists('check_points'):\n",
    "            os.mkdir('check_points')\n",
    "        if not os.path.exists(os.path.join('check_points', timestamp)):\n",
    "            os.mkdir(os.path.join('check_points', timestamp))\n",
    "        model_path = os.path.join('check_points', timestamp, model_path)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    # if avg_vloss < best_vloss:\n",
    "    #     best_vloss = avg_vloss\n",
    "    #     model_path = 'model_{}_epoch{}'.format(timestamp, epoch_number + 1)\n",
    "    #     torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store the values in a sub-dictionary with epoch number as key\n",
    "    logdata[epoch_number + 1] = {\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"train_acc\": avg_acc,\n",
    "        \"val_loss\": avg_vloss.tolist(),\n",
    "        \"val_acc\": avg_vacc.tolist()\n",
    "    }\n",
    "    \n",
    "    epoch_number += 1\n",
    "\n",
    "\n",
    "feeds = []\n",
    "# Write the dictionary object to your log file as JSON\n",
    "if not os.path.isfile(logfile):\n",
    "    json.dump(logdata, logfile)\n",
    "else:\n",
    "    with open(logfile) as feedsjson:\n",
    "        feeds = json.load(feedsjson)\n",
    "    for k,v in logdata.items():\n",
    "        feeds[k] = v\n",
    "    with open(logfile, mode='w') as f:\n",
    "        f.write(json.dumps(feeds, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train ver1\n",
    "# model.train() \n",
    "# train_loss = 0.0\n",
    "# train_acc = 0.0\n",
    "# for batch_idx, sample in tqdm(\n",
    "#     enumerate(train_dataloader), total=len(train_dataloader), leave=False\n",
    "# ):\n",
    "#     batch_img1, batch_img2, targets = sample[0].cuda(non_blocking=True), sample[1].cuda(non_blocking=True), sample[2].cuda(non_blocking=True) # or something similar\n",
    "#     targets = targets.unsqueeze(1)\n",
    "#     targets = targets.float()\n",
    "    \n",
    "#     # compute output, measure accuracy and record loss\n",
    "#     outputs = model(batch_img1, batch_img2)\n",
    "#     loss = loss_func(outputs, targets)\n",
    "#     acc = accuracy(outputs, targets)[0]\n",
    "#     acc = acc.sum() / len(acc)\n",
    "    \n",
    "#     # acc = eval_metrics(targets, outputs, 2)[0]\n",
    "\n",
    "#     break\n",
    "#     train_loss += loss.item()\n",
    "#     train_acc += acc.item()\n",
    "\n",
    "    \n",
    "#     # compute gradient and do SGD step\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybert",
   "language": "python",
   "name": "pybert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "92b0ced2bda8a9d74b5fcf92aa8bdcb1cdcbfa04a2239101742f1cb36a3c525b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
