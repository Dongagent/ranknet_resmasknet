{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a567fcf-d4c0-40b1-bf12-79a453741adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05d87a76-7a9d-431d-9383-12132f4fba36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [101]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_resource_path\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mResMaskNet\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feat'"
     ]
    }
   ],
   "source": [
    "\n",
    "from feat.utils import get_resource_path\n",
    "class ResMaskNet:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize ResMaskNet\n",
    "\n",
    "        @misc{luanresmaskingnet2020,\n",
    "        Author = {Luan Pham & Tuan Anh Tran},\n",
    "        Title = {Facial Expression Recognition using Residual Masking Network},\n",
    "        url = {https://github.com/phamquiluan/ResidualMaskingNetwork},\n",
    "        Year = {2020}\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), transforms.ToTensor()]\n",
    "        )\n",
    "\n",
    "        self.FER_2013_EMO_DICT = {\n",
    "            0: \"angry\",\n",
    "            1: \"disgust\",\n",
    "            2: \"fear\",\n",
    "            3: \"happy\",\n",
    "            4: \"sad\",\n",
    "            5: \"surprise\",\n",
    "            6: \"neutral\",\n",
    "        }\n",
    "\n",
    "        # load configs and set random seed\n",
    "        configs = json.load(\n",
    "            open(os.path.join(get_resource_path(), \"ResMaskNet_fer2013_config.json\"))\n",
    "        )\n",
    "        self.image_size = (configs[\"image_size\"], configs[\"image_size\"])\n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        # if self.use_gpu:\n",
    "        #     self.state = torch.load(\n",
    "        #         os.path.join(\n",
    "        #             get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "        #         )\n",
    "        #     )\n",
    "        # else:\n",
    "        #     self.state = torch.load(\n",
    "        #         os.path.join(\n",
    "        #             get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "        #         ),\n",
    "        #         map_location={\"cuda:0\": \"cpu\"},\n",
    "        #     )\n",
    "\n",
    "        self.model = resmasking_dropout1(in_channels=3, num_classes=7)\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(\n",
    "                        get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "                        )\n",
    "                    )['net']\n",
    "                )\n",
    "            self.model.cuda()\n",
    "\n",
    "        else:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    os.path.join(\n",
    "                        get_resource_path(), \"ResMaskNet_Z_resmasking_dropout1_rot30.pth\"\n",
    "                    ),\n",
    "                map_location={\"cuda:0\": \"cpu\"},\n",
    "                )['net']\n",
    "            )\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def detect_emo(self, frame, detected_face, *args, **kwargs):\n",
    "        \"\"\"Detect emotions.\n",
    "\n",
    "        Args:\n",
    "            frame ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            List of predicted emotions in probability: [angry, disgust, fear, happy, sad, surprise, neutral]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            frame = np.fliplr(frame).astype(np.uint8)\n",
    "            h, w = frame.shape[:2]\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            start_x, start_y, end_x, end_y, conf = np.array(detected_face[0]).astype(\n",
    "                int\n",
    "            )\n",
    "            # covnert to square images\n",
    "            center_x, center_y = (start_x + end_x) // 2, (start_y + end_y) // 2\n",
    "            square_length = ((end_x - start_x) + (end_y - start_y)) // 2 // 2\n",
    "            square_length *= 1.1\n",
    "            start_x = int(center_x - square_length)\n",
    "            start_y = int(center_y - square_length)\n",
    "            end_x = int(center_x + square_length)\n",
    "            end_y = int(center_y + square_length)\n",
    "            if start_x < 0:\n",
    "                start_x = 0\n",
    "            if start_y < 0:\n",
    "                start_y = 0\n",
    "            face = gray[start_y:end_y, start_x:end_x]\n",
    "            face = ensure_color(face)\n",
    "            face = cv2.resize(face, self.image_size)\n",
    "            if self.use_gpu:\n",
    "                face = self.transform(face).cuda()\n",
    "            else:\n",
    "                face = self.transform(face)\n",
    "            face = torch.unsqueeze(face, dim=0)\n",
    "            output = torch.squeeze(self.model(face), 0)\n",
    "            proba = torch.softmax(output, 0)\n",
    "            proba_np = proba.cpu().numpy()\n",
    "            return [proba_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c16b9f-5818-474a-9bf6-fbcb95d77ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pybert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "# from torch.nn.parallel import DistributedDataParallel\n",
    "from resmasknet_test import *\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import json\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from human_data import feat_order\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b64286-a06d-4f70-ba67-b911060cc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRankNet(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(SiameseRankNet, self).__init__()\n",
    "        # Load ResMaskNet model\n",
    "        # self.model = resmasking_dropout1(in_channels=3, num_classes=7)\n",
    "        self.model = ResMasking('')\n",
    "        # freeze\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # unfreeze\n",
    "        # Define the fully connected layers on top of concatenated feature vectors\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "        \n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        self.image_size = 224\n",
    "\n",
    "        self.FER_2013_EMO_DICT = {\n",
    "            0: \"angry\",\n",
    "            1: \"disgust\",\n",
    "            2: \"fear\",\n",
    "            3: \"happy\",\n",
    "            4: \"sad\",\n",
    "            5: \"surprise\",\n",
    "            6: \"neutral\",\n",
    "        }\n",
    "        self.FER_2013_EMONUM = {v:k for k, v in self.FER_2013_EMO_DICT.items()}\n",
    "        self.emotion = 'happy'\n",
    "        self.idx = self.FER_2013_EMONUM[self.emotion]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.activation = nn.Tanh()\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.state_dict = torch.load(self.model_path)\n",
    "            for key in list(self.state_dict.keys()):\n",
    "                self.state_dict[key.replace(\"module.model.\",\"\")] = self.state_dict.pop(key)\n",
    "            self.model.load_state_dict(self.state_dict)\n",
    "            self.model.cuda()\n",
    "\n",
    "        else:\n",
    "            self.state_dict = torch.load(self.model_path, map_location={\"cuda:0\": \"cpu\"},)\n",
    "            for key in list(self.state_dict.keys()):\n",
    "                self.state_dict[key.replace(\"module.model.\",\"\")] = self.state_dict.pop(key)\n",
    "            self.model.load_state_dict()\n",
    "        self.model.eval()\n",
    "        \n",
    "    def detect_emo(self, frame, detected_face=\"\", *args, **kwargs):\n",
    "        \"\"\"Detect emotions.\n",
    "\n",
    "        Args:\n",
    "            frame ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            List of predicted emotions in probability: [angry, disgust, fear, happy, sad, surprise, neutral]\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # frame = np.fliplr(frame).astype(np.uint8)\n",
    "            # h, w = frame.shape[:2]\n",
    "            # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # start_x, start_y, end_x, end_y, conf = np.array(detected_face[0]).astype(\n",
    "            #     int\n",
    "            # )\n",
    "\n",
    "            # test\n",
    "            start_x, start_y, end_x, end_y = 193, 114, 442, 363\n",
    "\n",
    "            # # covnert to square images\n",
    "            # center_x, center_y = (start_x + end_x) // 2, (start_y + end_y) // 2\n",
    "            # square_length = ((end_x - start_x) + (end_y - start_y)) // 2 // 2\n",
    "            # square_length *= 1.1\n",
    "            # start_x = int(center_x - square_length)\n",
    "            # start_y = int(center_y - square_length)\n",
    "            # end_x = int(center_x + square_length)\n",
    "            # end_y = int(center_y + square_length)\n",
    "            # if start_x < 0:\n",
    "            #     start_x = 0\n",
    "            # if start_y < 0:\n",
    "            #     start_y = 0\n",
    "            # face = gray[start_y:end_y, start_x:end_x]\n",
    "            # face = ensure_color(face)\n",
    "            # face = cv2.resize(face, (self.image_size, self.image_size))\n",
    "    \n",
    "            face = frame.crop([start_x, start_y, end_x, end_y])\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            if self.use_gpu:\n",
    "                face = self.transform(face).cuda()\n",
    "            else:\n",
    "                face = self.transform(face)\n",
    "            face = torch.unsqueeze(face, dim=0)\n",
    "            output = torch.squeeze(self.model(face), 0)\n",
    "            # proba = torch.softmax(output, 0)\n",
    "            # proba_np = proba.cpu().numpy()\n",
    "            return output\n",
    "\n",
    "\n",
    "\n",
    "path1 = \"check_points/Happy_Rank1_50_v1/20230809_170453/model_20230809_170453_epoch2.pt\"\n",
    "model = SiameseRankNet(path1)\n",
    "# model2 = ResMaskNet()\n",
    "\n",
    "# print(model.detect_emo(Image.open('data/happiness_selected_imgonly100/ha_1.png')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20e80c62-6185-47da-8db6-5aef8933d501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=False)\n",
       "  (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc5ed72-5126-42e3-adb9-12b71389e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0516, device='cuda:0')\n",
      "tensor(-0.0038, device='cuda:0')\n",
      "tensor(-0.1297, device='cuda:0')\n",
      "tensor(-0.0667, device='cuda:0')\n",
      "tensor(0.0272, device='cuda:0')\n",
      "tensor(-0.0012, device='cuda:0')\n",
      "tensor(-0.0682, device='cuda:0')\n",
      "tensor(-0.0259, device='cuda:0')\n",
      "tensor(-0.0088, device='cuda:0')\n",
      "tensor(-0.0513, device='cuda:0')\n",
      "tensor(0.0209, device='cuda:0')\n",
      "tensor(-0.0256, device='cuda:0')\n",
      "tensor(-0.1062, device='cuda:0')\n",
      "tensor(-0.0513, device='cuda:0')\n",
      "tensor(-0.0970, device='cuda:0')\n",
      "tensor(-0.0205, device='cuda:0')\n",
      "tensor(-0.1076, device='cuda:0')\n",
      "tensor(-0.0797, device='cuda:0')\n",
      "tensor(-0.1449, device='cuda:0')\n",
      "tensor(-0.0703, device='cuda:0')\n",
      "tensor(0.0256, device='cuda:0')\n",
      "tensor(-0.0369, device='cuda:0')\n",
      "tensor(0.1354, device='cuda:0')\n",
      "tensor(-0.0338, device='cuda:0')\n",
      "tensor(-0.1594, device='cuda:0')\n",
      "tensor(-0.0220, device='cuda:0')\n",
      "tensor(-0.1737, device='cuda:0')\n",
      "tensor(0.0482, device='cuda:0')\n",
      "tensor(-0.1014, device='cuda:0')\n",
      "tensor(-0.1305, device='cuda:0')\n",
      "tensor(-0.1469, device='cuda:0')\n",
      "tensor(-0.0128, device='cuda:0')\n",
      "tensor(-0.1735, device='cuda:0')\n",
      "tensor(-0.2060, device='cuda:0')\n",
      "tensor(0.0028, device='cuda:0')\n",
      "tensor(-0.1764, device='cuda:0')\n",
      "tensor(-0.2382, device='cuda:0')\n",
      "tensor(-0.1564, device='cuda:0')\n",
      "tensor(-0.0033, device='cuda:0')\n",
      "tensor(-0.1013, device='cuda:0')\n",
      "tensor(-0.2187, device='cuda:0')\n",
      "tensor(-0.1575, device='cuda:0')\n",
      "tensor(-0.1677, device='cuda:0')\n",
      "tensor(-0.0054, device='cuda:0')\n",
      "tensor(-0.1246, device='cuda:0')\n",
      "tensor(0.0319, device='cuda:0')\n",
      "tensor(-0.1574, device='cuda:0')\n",
      "tensor(-0.2442, device='cuda:0')\n",
      "tensor(-0.2595, device='cuda:0')\n",
      "tensor(-0.1596, device='cuda:0')\n",
      "tensor(-0.0408, device='cuda:0')\n",
      "tensor(-0.2852, device='cuda:0')\n",
      "tensor(-0.1770, device='cuda:0')\n",
      "tensor(-0.3371, device='cuda:0')\n",
      "tensor(-0.3086, device='cuda:0')\n",
      "tensor(-0.1324, device='cuda:0')\n",
      "tensor(-0.4998, device='cuda:0')\n",
      "tensor(-0.3012, device='cuda:0')\n",
      "tensor(-0.2085, device='cuda:0')\n",
      "tensor(-0.2611, device='cuda:0')\n",
      "tensor(-0.2973, device='cuda:0')\n",
      "tensor(-0.3076, device='cuda:0')\n",
      "tensor(-0.3251, device='cuda:0')\n",
      "tensor(-0.4368, device='cuda:0')\n",
      "tensor(-0.2697, device='cuda:0')\n",
      "tensor(-0.2091, device='cuda:0')\n",
      "tensor(-0.1671, device='cuda:0')\n",
      "tensor(-0.4175, device='cuda:0')\n",
      "tensor(-0.1987, device='cuda:0')\n",
      "tensor(-0.2619, device='cuda:0')\n",
      "tensor(-0.2688, device='cuda:0')\n",
      "tensor(-0.3053, device='cuda:0')\n",
      "tensor(-0.1556, device='cuda:0')\n",
      "tensor(-0.1007, device='cuda:0')\n",
      "tensor(-0.4325, device='cuda:0')\n",
      "tensor(-0.2622, device='cuda:0')\n",
      "tensor(-0.3303, device='cuda:0')\n",
      "tensor(-0.3247, device='cuda:0')\n",
      "tensor(-0.1329, device='cuda:0')\n",
      "tensor(-0.3288, device='cuda:0')\n",
      "tensor(-0.4253, device='cuda:0')\n",
      "tensor(-0.3327, device='cuda:0')\n",
      "tensor(-0.4346, device='cuda:0')\n",
      "tensor(-0.3497, device='cuda:0')\n",
      "tensor(-0.2740, device='cuda:0')\n",
      "tensor(-0.3345, device='cuda:0')\n",
      "tensor(-0.2337, device='cuda:0')\n",
      "tensor(-0.4060, device='cuda:0')\n",
      "tensor(-0.4535, device='cuda:0')\n",
      "tensor(-0.2521, device='cuda:0')\n",
      "tensor(-0.3268, device='cuda:0')\n",
      "tensor(-0.4564, device='cuda:0')\n",
      "tensor(-0.4307, device='cuda:0')\n",
      "tensor(-0.2657, device='cuda:0')\n",
      "tensor(-0.4542, device='cuda:0')\n",
      "tensor(-0.3711, device='cuda:0')\n",
      "tensor(-0.4051, device='cuda:0')\n",
      "tensor(-0.4320, device='cuda:0')\n",
      "tensor(-0.4282, device='cuda:0')\n",
      "tensor(-0.4193, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "aver_4_sorted_data = ['ha_428.png', 'ha_393.png', 'ha_407.png', 'ha_422.png', 'ha_341.png', 'ha_202.png', 'ha_235.png', 'ha_288.png', 'ha_50.png', 'ha_489.png', 'ha_212.png', 'ha_450.png', 'ha_348.png', 'ha_24.png', 'ha_443.png', 'ha_256.png', 'ha_412.png', 'ha_185.png', 'ha_301.png', 'ha_11.png', 'ha_201.png', 'ha_497.png', 'ha_384.png', 'ha_27.png', 'ha_184.png', 'ha_368.png', 'ha_268.png', 'ha_442.png', 'ha_491.png', 'ha_367.png', 'ha_311.png', 'ha_19.png', 'ha_243.png', 'ha_249.png', 'ha_215.png', 'ha_107.png', 'ha_130.png', 'ha_221.png', 'ha_303.png', 'ha_342.png', 'ha_244.png', 'ha_2.png', 'ha_54.png', 'ha_320.png', 'ha_32.png', 'ha_329.png', 'ha_241.png', 'ha_315.png', 'ha_33.png', 'ha_451.png', 'ha_151.png', 'ha_5.png', 'ha_476.png', 'ha_296.png', 'ha_344.png', 'ha_1.png', 'ha_152.png', 'ha_132.png', 'ha_172.png', 'ha_207.png', 'ha_377.png', 'ha_131.png', 'ha_404.png', 'ha_149.png', 'ha_477.png', 'ha_466.png', 'ha_454.png', 'ha_186.png', 'ha_122.png', 'ha_106.png', 'ha_124.png', 'ha_43.png', 'ha_292.png', 'ha_380.png', 'ha_284.png', 'ha_59.png', 'ha_110.png', 'ha_396.png', 'ha_382.png', 'ha_475.png', 'ha_334.png', 'ha_119.png', 'ha_482.png', 'ha_381.png', 'ha_9.png', 'ha_252.png', 'ha_174.png', 'ha_145.png', 'ha_116.png', 'ha_314.png', 'ha_128.png', 'ha_310.png', 'ha_262.png', 'ha_257.png', 'ha_139.png', 'ha_84.png', 'ha_233.png', 'ha_399.png', 'ha_111.png', 'ha_16.png']\n",
    "\n",
    "# test for pyfeat ranking\n",
    "cleaned_data = feat_order['happy'][:5]\n",
    "cleaned_data = feat_order['happy']\n",
    "\n",
    "final_res = []\n",
    "for i in cleaned_data:\n",
    "    tmp_path = os.path.join('data/happiness_selected_imgonly100', i)\n",
    "    tmp_res = {}\n",
    "    for k,v in model.FER_2013_EMO_DICT.items():\n",
    "        # print(k)\n",
    "        # print(model.detect_emo(Image.open(tmp_path)))\n",
    "        tmp_res[v] = list(model.detect_emo(Image.open(tmp_path)))[k]\n",
    "    print(tmp_res['happy'])\n",
    "    final_res.append(tmp_res['happy'].tolist())\n",
    "    # print(tmp_res)\n",
    "    # print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "681a86c1-f6c1-46ab-8006-36d5fe5a6c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22, 0.13539372384548187], [27, 0.048190079629421234], [45, 0.03191537782549858], [4, 0.027175672352313995], [20, 0.025602273643016815], [10, 0.02091219276189804], [34, 0.002751395106315613], [5, -0.0011971443891525269], [38, -0.0032579824328422546], [1, -0.0037516728043556213], [43, -0.005445666611194611], [8, -0.008805736899375916], [31, -0.012804258614778519], [15, -0.020520687103271484], [25, -0.0219891294836998], [11, -0.02563406527042389], [7, -0.025892414152622223], [23, -0.033829618245363235], [21, -0.036931782960891724], [50, -0.04082949087023735], [9, -0.051255807280540466], [13, -0.05126189440488815], [0, -0.05163617432117462], [3, -0.06668658554553986], [6, -0.06819435209035873], [19, -0.07032498717308044], [17, -0.07974196970462799], [14, -0.0970403403043747], [73, -0.10070277750492096], [39, -0.10132899880409241], [28, -0.10137064009904861], [12, -0.10623828321695328], [16, -0.10763641446828842], [44, -0.12455587834119797], [2, -0.12971627712249756], [29, -0.13053488731384277], [55, -0.13235491514205933], [78, -0.13286399841308594], [18, -0.14487683773040771], [30, -0.14694063365459442], [72, -0.155599445104599], [37, -0.15644118189811707], [46, -0.15742865204811096], [41, -0.1575247198343277], [24, -0.15939749777317047], [49, -0.15960705280303955], [66, -0.16710607707500458], [42, -0.16765807569026947], [32, -0.17345918715000153], [26, -0.17373526096343994], [35, -0.17639945447444916], [52, -0.17704352736473083], [68, -0.198713481426239], [33, -0.2059621661901474], [58, -0.20852267742156982], [65, -0.20910660922527313], [40, -0.21867647767066956], [86, -0.23365797102451324], [36, -0.23817309737205505], [47, -0.2441522628068924], [89, -0.25205206871032715], [48, -0.2595122456550598], [59, -0.26110589504241943], [69, -0.2619132399559021], [75, -0.2621553838253021], [93, -0.26570236682891846], [70, -0.26881515979766846], [64, -0.26970577239990234], [84, -0.2739652395248413], [51, -0.2851768732070923], [60, -0.2972937226295471], [57, -0.301219642162323], [71, -0.30529671907424927], [61, -0.3075815439224243], [54, -0.3086296319961548], [77, -0.32469868659973145], [62, -0.3250945806503296], [90, -0.3267917037010193], [79, -0.32877933979034424], [76, -0.3303258419036865], [81, -0.3327449560165405], [85, -0.3344566822052002], [53, -0.33712220191955566], [83, -0.34967178106307983], [95, -0.3711455464363098], [96, -0.40505504608154297], [87, -0.40595144033432007], [67, -0.41753482818603516], [99, -0.41925692558288574], [80, -0.42527687549591064], [98, -0.42818784713745117], [92, -0.43068647384643555], [97, -0.4320406913757324], [74, -0.4324949383735657], [82, -0.43462443351745605], [63, -0.43682897090911865], [88, -0.45350736379623413], [94, -0.4541929364204407], [91, -0.456400990486145], [56, -0.499817430973053]]\n",
      "['ha_384.png', 'ha_442.png', 'ha_329.png', 'ha_341.png', 'ha_201.png', 'ha_212.png', 'ha_215.png', 'ha_202.png', 'ha_303.png', 'ha_393.png', 'ha_320.png', 'ha_50.png', 'ha_19.png', 'ha_256.png', 'ha_368.png', 'ha_450.png', 'ha_288.png', 'ha_27.png', 'ha_497.png', 'ha_151.png', 'ha_489.png', 'ha_24.png', 'ha_428.png', 'ha_422.png', 'ha_235.png', 'ha_11.png', 'ha_185.png', 'ha_443.png', 'ha_380.png', 'ha_342.png', 'ha_491.png', 'ha_348.png', 'ha_412.png', 'ha_32.png', 'ha_407.png', 'ha_367.png', 'ha_1.png', 'ha_382.png', 'ha_301.png', 'ha_311.png', 'ha_292.png', 'ha_221.png', 'ha_241.png', 'ha_2.png', 'ha_184.png', 'ha_451.png', 'ha_454.png', 'ha_54.png', 'ha_243.png', 'ha_268.png', 'ha_107.png', 'ha_476.png', 'ha_122.png', 'ha_249.png', 'ha_172.png', 'ha_466.png', 'ha_244.png', 'ha_174.png', 'ha_130.png', 'ha_315.png', 'ha_314.png', 'ha_33.png', 'ha_207.png', 'ha_106.png', 'ha_59.png', 'ha_257.png', 'ha_124.png', 'ha_477.png', 'ha_9.png', 'ha_5.png', 'ha_377.png', 'ha_132.png', 'ha_43.png', 'ha_131.png', 'ha_344.png', 'ha_396.png', 'ha_404.png', 'ha_128.png', 'ha_475.png', 'ha_110.png', 'ha_119.png', 'ha_252.png', 'ha_296.png', 'ha_381.png', 'ha_84.png', 'ha_233.png', 'ha_145.png', 'ha_186.png', 'ha_16.png', 'ha_334.png', 'ha_111.png', 'ha_262.png', 'ha_399.png', 'ha_284.png', 'ha_482.png', 'ha_149.png', 'ha_116.png', 'ha_139.png', 'ha_310.png', 'ha_152.png']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_rank = [[i, v] for i, v in enumerate(final_res)]\n",
    "\n",
    "final_rank.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "print(final_rank)\n",
    "fin = [aver_4_sorted_data[i[0]] for i in final_rank]\n",
    "print(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7880cf84-8d60-4eae-87d7-47fbd38e1e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09543101\n",
      "\n",
      "0.09662038\n",
      "\n",
      "0.10928758\n",
      "\n",
      "0.10535402\n",
      "\n",
      "0.090404704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test for pyfeat ranking\n",
    "cleaned_data = feat_order['happy'][-5:]\n",
    "\n",
    "for i in cleaned_data:\n",
    "    tmp_path = os.path.join('data/happiness_selected_imgonly100', i)\n",
    "    tmp_res = {}\n",
    "    for k,v in model.FER_2013_EMO_DICT.items():\n",
    "        # print(k)\n",
    "        # print(model.detect_emo(Image.open(tmp_path)))\n",
    "        tmp_res[v] = list(model.detect_emo(Image.open(tmp_path)))[k]\n",
    "    print(tmp_res['happy'])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5f9822e8-b96e-4fff-8453-c59d9c48cc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0.07773054,\n",
       " 'disgust': 0.21724878,\n",
       " 'fear': 0.22108388,\n",
       " 'happy': 0.090404704,\n",
       " 'sad': 0.082979575,\n",
       " 'surprise': 0.17381632,\n",
       " 'neutral': 0.13673624}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d773a42f-16d9-4cf8-89e8-50df4721d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000447034836"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tmp_res.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813af302-ff7c-4035-983f-53660fbd7c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybert",
   "language": "python",
   "name": "pybert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
